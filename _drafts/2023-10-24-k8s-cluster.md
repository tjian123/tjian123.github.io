---
layout: default
title: k8s 高可用集群环境搭建
description: 本文在 Windows 11 主机环境下，基于 VirtualBox 搭建了最新版的 k8s（v1.28.2） 高可用集群环境。本文主要记录了环境搭建过程，以及部分踩坑经验。
categories: 
   - 技术积累
tags: 
   - k8s
---

> 本文在 Windows 11 主机环境下，基于 VirtualBox 搭建了最新版的 k8s（v1.28.2） 高可用集群环境。本文主要记录了环境搭建过程，以及部分踩坑经验。

<!-- more -->
* TOC
{:toc}

## 1. 准备节点环境

本文环境是在一台 Windows 11 的主机环境下搭建完成，各 k8s 节点基于虚拟机提供，节点环境为 Debian 12(没有选择 ubuntu 的原因是，ubuntu server 23.04 最新 server 安装版的 iso 镜像已经达到 2G+，相比之下 debian 12 的 iso 镜像只需要不到 700M)。

准备节点环境阶段，我们将完成节点系统安装、主机和网络配置，以及配置本地主机到所有节点的 ssh 免密登录。

### 1.1 下载和安装虚拟机软件

本文安装的 VirtualBox 7.0，官网[下载](https://www.virtualbox.org/wiki/Downloads)最新版可执行文件，点击安装即可。

**注 1**：安装时会指定软件安装目录和系统存放目录，默认都在 C 盘，建议放在其他存放空间足够的磁盘。

**注 2**：如果是其他环境，在安装了 VirtualBox 后可以考虑使用 vagrant 来初始化虚拟机节点，这样安装过程将更加可重复、可记录、可维护。（*本文实践过程中非常遗憾无法通过 vagrant 成功创建虚拟机，多方查证后在 vagrant 和 virtualBox 的 github 仓库的 issuls 中都找到了有人提过的记录，但从回答来看，两者都不认为是自己的问题，后来有网友提出该问题仅在 insider 系统下出现，于是放弃了基于 vagrant 折腾。*）

### 1.2 下载 debian 系统 iso 镜像

从国内镜像源[下载](https://mirrors.163.com/debian-cd/12.2.0/amd64/iso-cd/) debian iso 镜像，以备使用。

**注 3**：CD 版和 DVD 版是两种分发方式，CD 版容量为 700M，DVD 版容量为 4.7G，CD 版系统盘一般仅包含必要核心模块，而 DVD 版则是比较全面的系统。

### 1.3 安装 debian 系统虚拟机

因为是搭建高可用集群，至少是两个 master 节点，本文实验的是 3 个 master 节点 + 3 个 worker 节点，主机和网络规划如下：

|虚拟主机名称|静态 IP|
|-----------|-------|
|k8s-master1|192.168.56.20|
|k8s-master2|192.168.56.21|
|k8s-master3|192.168.56.22|
|k8s-node1  |192.168.56.25|
|k8s-node2  |192.168.56.26|
|k8s-node3  |192.168.56.27|

安装一个虚拟机节点的步骤如下：

1. 打开 VirtualBox 软件
2. 点击【新建】按钮
3. 在对话框的【名称】栏填写主机名称，如 k8s-master1
4. 【文件夹】栏指向虚拟机节点存放位置，如需修改则修改到自定义位置
5. 【类型栏】选择 【Linux】
6. 【版本栏】选择【Debian (64-bit)】
7. 点击【下一步】，内存配置为 2048M，CPU 配置为 2 核
8. 点击【下一步】，硬盘空间配置为 10G
9. 点击【下一步】，点击【完成】，关闭对话框
10. 在 VirtualBox 软件侧边栏选中刚创建的虚拟机，在右侧存储位置会看到有个【控制器：IDE】的选项，点击下方的【没有盘片】，点击【选择虚拟盘】选中在 1.2 中下载的 debian iso 镜像
11. 点击上边栏靠右的【启动】按钮，启动虚拟机安装过程，该过程和普通系统安装过程一致，过程略

**注**：安装玩一个虚拟机后，可以按照同样的方式来安装其他虚拟机，但更快的方式是拷贝复制。直接在 VirtualBox 软件侧边栏选中刚刚完成系统安装的虚拟机，右键选择【复制】后改名即可（源虚拟机关闭状态下才可以被复制）

#### 配置静态 IP

以 k8s-master1 为例配置静态 IP 和 NAT 对应的网卡，修改 `/etc/network/interfaces` 文件如下：

```shell
# cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
#allow-hotplug enp0s3
#iface enp0s3 inet dhcp

auto enp0s3
iface enp0s3 inet static
        address 192.168.56.20/24

allow-hotplug enp0s8
iface enp0s8 inet dhcp
```

修改后通过 `sudo systemctl restart networking` 重启 networking 服务生效，可以通过 `ip a s` 命令验证。

## 2. 配置节点环境

安装 containerd 容器运行时环境

```shell
#删除旧包，这里不需要，因为是全新安装
apt-get remove docker docker-engine docker.io containerd runc

# 安装新源
apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

# 下载密钥
mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg

# 添加新源
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \
  $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

# 更新源
apt-get update

# 安装containerd
apt-get install containerd
```

查看安装过程，发现会自动安装 runc

```shell
...
Unpacking runc (1.1.5+ds1-1+b1) ...
...
```

安装的 runc 是 1.1.5+ds1-1+b1，github 最新版是 1.1.9
而 containerd 是 1.6.20~ds1-1+b1，github 最新版是 1.7.7

所以如果需要最新版，需要基于源码安装等其他方式

安装后刚起来的配置

```shell
#cat /etc/containerd/config.toml
version = 2

[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/usr/lib/cni"
      conf_dir = "/etc/cni/net.d"
  [plugins."io.containerd.internal.v1.opt"]
    path = "/var/lib/containerd/opt"
```

默认配置是这样的 `containerd config default`

```shell
disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_ca = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.k8s.io/pause:3.6"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
      ip_pref = ""
      max_conf_num = 1

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      ignore_rdt_not_enabled_errors = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = false

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"

  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]
    sched_core = false

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]

  [plugins."io.containerd.service.v1.tasks-service"]
    rdt_config_file = ""

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    discard_blocks = false
    fs_options = ""
    fs_type = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    root_path = ""
    upperdir_label = false

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.bolt.open" = "0s"
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0
```

配置systemd为cgroup驱动程序

在 /etc/containerd/config.toml 中设置

```shell
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
```

containerd 命令行

* `ctr i pull <image>` 下载镜像
* `ctr i ls` 查看镜像
* `ctr c create <image> <name>` 创建容器，创建后的容器还是未运行的
* `ctr c ls` 查看容器列表
* `ctr c rm <name>` 删除指定容器
* `ctr task start -d <name>` 启动一个任务，启用之前创建的容器
* `ctr task ls` 查看任务列表
* `ctr run -d <image> <name>` 一条命令同时创建和启动容器
* `ctr task kill <name>` 杀死任务
* `ctr task ls` 查看任务列表
* `ctr ns ls` 查看命名空间

## 使用 kubeadm 安装 k8s 集群主节点

**确保节点的 MAC 地址唯一**，使用 `ip link` 或者 `ifconfig -a` 来获取网络接口的 MAC 地址
**确保节点的 product_uuid 唯一**，使用 `cat /sys/class/dmi/id/product_uuid` 查看

```

Your Kubernetes control-plane has initialized successfully!                                                          
                                                                                                                     
To start using your cluster, you need to run the following as a regular user:                                        
                                                                                                                     
  mkdir -p $HOME/.kube                                                                                               
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config                                                           
  sudo chown $(id -u):$(id -g) $HOME/.kube/config                                                                    
                                                                                                                     
Alternatively, if you are the root user, you can run:                                                                
                                                                                                                     
  export KUBECONFIG=/etc/kubernetes/admin.conf                                                                       
                                                                                                                     
You should now deploy a pod network to the cluster.                                                                  
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:                                          
  https://kubernetes.io/docs/concepts/cluster-administration/addons/                                                 
                                                                                                                     
You can now join any number of the control-plane node running the following command on each as root:                 
                                                                                                                     
  kubeadm join 192.168.56.100:9443 --token qnzxbr.tdw5pt8haef8slws --discovery-token-ca-cert-hash sha256:b1bc35b5fe6f7b57413e67af058c4290969de66dee4a52997db4421ab5e6208d --control-plane --certificate-key 112652d05454816f65e0d1768d0884bb11bb5cd509cb1d2174d35960713f6257  --apiserver-advertise-address=192.168.56.2       
                                                                                                                     
Please note that the certificate-key gives access to cluster sensitive data, keep it secret!                         
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use                               
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.                                          
                                                                                                                     
Then you can join any number of worker nodes by running the following on each as root:                               
                                                                                                                     
kubeadm join 192.168.56.100:9443 --token qnzxbr.tdw5pt8haef8slws --discovery-token-ca-cert-hash sha256:b1bc35b5fe6f7b57413e67af058c4290969de66dee4a52997db4421ab5e6208d       
```

  kubeadm join 192.168.56.100:9443 --token snqg7x.1e1rplcyuc4trivw \
        --discovery-token-ca-cert-hash sha256:ef38fde82091fa408e1f598753a7cbab031e711f8ab20bdcf459a39e0f0862a6 \
        --control-plane --certificate-key dcda037c006f31a22c85e942f78f0be208da670bdb8bdcf6c751a2a2bba0d242 \
        --apiserver-advertise-address=192.168.56.2

  kubeadm join 192.168.56.100:9443 --token j3ftzm.qn1bgl0ghtv1bafr \
        --discovery-token-ca-cert-hash sha256:ef38fde82091fa408e1f598753a7cbab031e711f8ab20bdcf459a39e0f0862a6 \
        --control-plane --certificate-key dcda037c006f31a22c85e942f78f0be208da670bdb8bdcf6c751a2a2bba0d242 \
        --apiserver-advertise-address=192.168.56.2

kubeadm join 192.168.56.20:9443 --token snqg7x.1e1rplcyuc4trivw \
        --discovery-token-ca-cert-hash sha256:ef38fde82091fa408e1f598753a7cbab031e711f8ab20bdcf459a39e0f0862a6

查看token `kubeadm token list`
